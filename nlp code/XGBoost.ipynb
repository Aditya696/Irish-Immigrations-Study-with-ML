{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73798475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.classify.util as util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder as BCF\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import os.path\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a692413",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {}\n",
    "with open('emoji.txt', 'r', encoding='latin-1') as emoji_file:\n",
    "    for line in emoji_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            emoji, value = line.split('\\t')\n",
    "            emoji_dict[emoji] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0f3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def replace_emojis(text, emoji_dict):\n",
    "    for emoji, value in emoji_dict.items():\n",
    "        if value == 1:\n",
    "            text = re.sub(re.escape(emoji), 'happy', text)\n",
    "        elif value == -1:\n",
    "            text = re.sub(re.escape(emoji), 'sad', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac4c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_preprocessed = []\n",
    "with open('positive.txt', 'r', encoding='latin-1') as pos_file:\n",
    "    for line in pos_file:\n",
    "        comment = line.strip()\n",
    "        comment = replace_emojis(comment, emoji_dict)\n",
    "        comment = preprocess_text(comment)\n",
    "        positive_preprocessed.append(comment)\n",
    "\n",
    "# Preprocess negative text\n",
    "negative_preprocessed = []\n",
    "with open('negative.txt', 'r', encoding='latin-1') as neg_file:\n",
    "    for line in neg_file:\n",
    "        comment = line.strip()\n",
    "        comment = replace_emojis(comment, emoji_dict)\n",
    "        comment = preprocess_text(comment)\n",
    "        negative_preprocessed.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1eec918",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4133839e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04384504,  0.06782313, -0.01133135, ..., -0.0765904 ,\n",
       "        -0.01084682, -0.01331438],\n",
       "       [-0.04973602,  0.00181389, -0.03222084, ..., -0.13389587,\n",
       "         0.00722504, -0.00415039],\n",
       "       [ 0.1281128 , -0.1151123 , -0.00805664, ...,  0.05335999,\n",
       "        -0.07922363,  0.31835938],\n",
       "       ...,\n",
       "       [ 0.03812218,  0.10181274, -0.03788071, ..., -0.11651306,\n",
       "        -0.00327454,  0.02767296],\n",
       "       [ 0.05984497,  0.10308838, -0.07188034, ...,  0.03323364,\n",
       "        -0.01623535,  0.06933594],\n",
       "       [ 0.3063151 ,  0.22135417,  0.01839193, ...,  0.05924479,\n",
       "        -0.052653  ,  0.03417969]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels\n",
    "positive_labels = np.ones(len(positive_preprocessed))\n",
    "negative_labels = np.zeros(len(negative_preprocessed))\n",
    "\n",
    "# Combine data and labels\n",
    "data = positive_preprocessed + negative_preprocessed\n",
    "labels = np.concatenate([positive_labels, negative_labels])\n",
    "\n",
    "# Convert text to word embeddings\n",
    "data_embeddings = []\n",
    "for text in data:\n",
    "    words = word_tokenize(text)\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in word_embedding_model:\n",
    "            embeddings.append(word_embedding_model[word])\n",
    "    if embeddings:\n",
    "        text_embedding = np.mean(embeddings, axis=0)\n",
    "        data_embeddings.append(text_embedding)\n",
    "\n",
    "data_embeddings = np.array(data_embeddings)\n",
    "data_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd3e99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12437773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7428437353355233\n"
     ]
    }
   ],
   "source": [
    "labels = labels[:data_embeddings.shape[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4c5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
