{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.classify.util as util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder as BCF\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import os.path\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21494d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {}\n",
    "with open('emoji.txt', 'r', encoding='latin-1') as emoji_file:\n",
    "    for line in emoji_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            emoji, value = line.split('\\t')\n",
    "            emoji_dict[emoji] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91106086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text, emoji_dict):\n",
    "    for emoji, value in emoji_dict.items():\n",
    "        if value == 1:\n",
    "            text = re.sub(re.escape(emoji), 'happy', text)\n",
    "        elif value == -1:\n",
    "            text = re.sub(re.escape(emoji), 'sad', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#vincent\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# def preprocess_text(text):\n",
    "#     # Remove special characters\n",
    "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = word_tokenize(text)\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     text = ' '.join(filtered_words)\n",
    "\n",
    "#     return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) if get_wordnet_pos(pos) else word for word, pos in tagged]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Preprocess positive text\n",
    "positive_preprocessed = []\n",
    "with open('positive.txt', 'r', encoding='latin-1') as pos_file:\n",
    "    for line in pos_file:\n",
    "        comment = line.strip()\n",
    "        comment = replace_emojis(comment, emoji_dict)\n",
    "        comment = preprocess_text(comment)\n",
    "        positive_preprocessed.append(comment)\n",
    "\n",
    "# Preprocess negative text\n",
    "negative_preprocessed = []\n",
    "with open('negative.txt', 'r', encoding='latin-1') as neg_file:\n",
    "    for line in neg_file:\n",
    "        comment = line.strip()\n",
    "        comment = replace_emojis(comment, emoji_dict)\n",
    "        comment = preprocess_text(comment)\n",
    "        negative_preprocessed.append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = positive_preprocessed + negative_preprocessed\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Create frequency distribution\n",
    "word_freq = FreqDist()\n",
    "\n",
    "for text in all_texts:\n",
    "    words = text.split()\n",
    "    word_freq.update(words)\n",
    "known_words = {word for word, freq in word_freq.items() if freq > 3}\n",
    "#len(known_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total_words = 0\n",
    "num_comments = len(all_texts)\n",
    "\n",
    "for comment in all_texts:\n",
    "    total_words += len(comment.split())\n",
    "\n",
    "average_words = total_words / num_comments\n",
    "\n",
    "variance = 0\n",
    "for comment in all_texts:\n",
    "    num_words = len(comment.split())\n",
    "    variance += (num_words - average_words) ** 2\n",
    "\n",
    "std_deviation = math.sqrt(variance / num_comments)\n",
    "M = round(average_words + std_deviation)\n",
    "\n",
    "def comment_to_vector(comment, known_words, M):\n",
    "    words = comment.split()\n",
    "    vector = []\n",
    "    for i in range(M):\n",
    "        if i < len(words):\n",
    "            word = words[i]\n",
    "            if word in known_words:\n",
    "                vector.append(known_words.index(word) + 1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "# Convert preprocessed comments to vector representation\n",
    "positive_comment_vectors = []\n",
    "for comment in positive_preprocessed:\n",
    "    vector = comment_to_vector(comment, list(known_words), M)\n",
    "    positive_comment_vectors.append(vector)\n",
    "    \n",
    "\n",
    "negative_comment_vectors = []\n",
    "for comment in negative_preprocessed:\n",
    "    vector = comment_to_vector(comment, list(known_words), M)\n",
    "    negative_comment_vectors.append(vector)\n",
    "positive_comment_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3251b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Flatten\n",
    "# import numpy as np\n",
    "\n",
    "# positive_comment_vectors = np.array(positive_comment_vectors)\n",
    "# negative_comment_vectors = np.array(negative_comment_vectors)\n",
    "\n",
    "# all_comment_vectors = np.concatenate((positive_comment_vectors, negative_comment_vectors))\n",
    "\n",
    "# positive_labels = np.ones(len(positive_comment_vectors))\n",
    "# negative_labels = np.zeros(len(negative_comment_vectors))\n",
    "\n",
    "# all_labels = np.concatenate((positive_labels, negative_labels))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_comment_vectors, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# embedding_dim = 300\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(known_words) + 1, output_dim=embedding_dim, input_length=M))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "# model.add(Dense(units=50, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(units=1, activation='tanh'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b6a63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b54864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # 更改预处理函数，添加词干提取和词形还原\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Remove special characters\n",
    "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = word_tokenize(text)\n",
    "#     # Add stemming and lemmatization\n",
    "#     filtered_words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word.lower() not in stop_words]\n",
    "#     text = ' '.join(filtered_words)\n",
    "#     return text\n",
    "\n",
    "# # Tokenization and padding\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(all_texts) # Use all available texts to build the token dictionary\n",
    "\n",
    "# X = tokenizer.texts_to_sequences(all_texts)\n",
    "# X = pad_sequences(X)  # Use default padding ('pre')\n",
    "\n",
    "# # Change the labels to a numpy array\n",
    "# Y = np.array(all_labels)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 优化模型结构，这里使用LSTM替换CNN\n",
    "# embedding_dim = 300\n",
    "# vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because 0 is used for padding\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X.shape[1]))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(32))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Using sigmoid as the activation function in the last layer, as this is a binary classification problem\n",
    "\n",
    "# # Using Adam optimizer that adjusts learning rate automatically\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Increasing the number of training epochs\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "positive_comment_vectors = np.array(positive_comment_vectors)\n",
    "negative_comment_vectors = np.array(negative_comment_vectors)\n",
    "\n",
    "all_comment_vectors = np.concatenate((positive_comment_vectors, negative_comment_vectors))\n",
    "\n",
    "positive_labels = np.ones(len(positive_comment_vectors))\n",
    "negative_labels = np.zeros(len(negative_comment_vectors))\n",
    "\n",
    "all_labels = np.concatenate((positive_labels, negative_labels))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_comment_vectors, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(known_words) + 1, output_dim=embedding_dim, input_length=M))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5)) # Dropout layer for regularization\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5)) # Dropout layer for regularization\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dropout(0.5)) # Dropout layer for regularization\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1, activation='tanh'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"embedding dimension = %d should be divisible by number of heads = %d\" % (embed_dim, num_heads))\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "embed_dim = 32  # Embedding size for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90babcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "class TransformerClassifier(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "        self.classifier = layers.Dense(1, activation='sigmoid', \n",
    "                                       kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Parameters for the model\n",
    "vocab_size = 20000  \n",
    "maxlen = 200  \n",
    "embed_dim = 32  \n",
    "num_heads = 2  \n",
    "ff_dim = 32  \n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerClassifier(vocab_size, maxlen, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
