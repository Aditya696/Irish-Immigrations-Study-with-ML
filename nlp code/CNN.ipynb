{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f93e9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thaku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\thaku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thaku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import os.path\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21494d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {}\n",
    "with open('emoji.txt', 'r', encoding='latin-1') as emoji_file:\n",
    "    for line in emoji_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            emoji, value = line.split('\\t')\n",
    "            emoji_dict[emoji] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91106086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text, emoji_dict):\n",
    "    for emoji, value in emoji_dict.items():\n",
    "        if value == 1:\n",
    "            text = re.sub(re.escape(emoji), 'happy', text)\n",
    "        elif value == -1:\n",
    "            text = re.sub(re.escape(emoji), 'sad', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ac01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vincent\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    words = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) if get_wordnet_pos(pos) else word for word, pos in tagged]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "hatefulCorpus=[]\n",
    "nHatefulCorpus=[]\n",
    "neutralCorpus=[]\n",
    "with open(\"firstIter.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if not all(value == \"\" for value in row.values()):\n",
    "            text = row[\"Comment Text\"]\n",
    "            if text == \"\":\n",
    "                continue\n",
    "            label = row[\"Label\"]\n",
    "            if label == \"\":\n",
    "                continue\n",
    "            try:\n",
    "                lang=detect(text)\n",
    "            except:\n",
    "                lang = \"\"\n",
    "            if lang != \"en\":\n",
    "                continue\n",
    "            newText = text.strip()\n",
    "            newText = replace_emojis(newText, emoji_dict)\n",
    "            newText = preprocess_text(newText)\n",
    "            if label=='Neutral':\n",
    "                neutralCorpus.append(newText)\n",
    "            elif label=='Hateful':\n",
    "                hatefulCorpus.append(newText)\n",
    "            else:\n",
    "                nHatefulCorpus.append(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa1c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_texts = hatefulCorpus + nHatefulCorpus + neutralCorpus\n",
    "# Create frequency distribution\n",
    "#word_freq = FreqDist()\n",
    "\n",
    "#for text in all_texts:\n",
    "#    words = text.split()\n",
    "#    word_freq.update(words)\n",
    "#known_words = {word for word, freq in word_freq.items() if freq > 3}\n",
    "#len(known_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7414aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_words = 0\n",
    "#num_comments = len(all_texts)\n",
    "\n",
    "#for comment in all_texts:\n",
    "#    total_words += len(comment.split())\n",
    "\n",
    "#average_words = total_words / num_comments\n",
    "\n",
    "#variance = 0\n",
    "#for comment in all_texts:\n",
    "#    num_words = len(comment.split())\n",
    "#    variance += (num_words - average_words) ** 2\n",
    "\n",
    "#std_deviation = math.sqrt(variance / num_comments)\n",
    "#M = round(average_words + std_deviation)\n",
    "\n",
    "#def comment_to_vector(comment, known_words, M):\n",
    "#    words = comment.split()\n",
    "#    vector = []\n",
    "#    for i in range(M):\n",
    "#        if i < len(words):\n",
    "#            word = words[i]\n",
    "#            if word in known_words:\n",
    "#                vector.append(known_words.index(word) + 1)\n",
    "#            else:\n",
    "#                vector.append(0)\n",
    "#        else:\n",
    "#            vector.append(0)\n",
    "#    return vector\n",
    "\n",
    "# Convert preprocessed comments to vector representation\n",
    "#hateful_comment_vectors = []\n",
    "#for comment in hatefulCorpus:\n",
    "#    vector = comment_to_vector(comment, list(known_words), M)\n",
    "#    hateful_comment_vectors.append(vector)\n",
    "    \n",
    "\n",
    "#nHateful_comment_vectors = []\n",
    "#for comment in nHatefulCorpus:\n",
    "#    vector = comment_to_vector(comment, list(known_words), M)\n",
    "#    nHateful_comment_vectors.append(vector)\n",
    "#    \n",
    "#neutral_comment_vectors = []\n",
    "#for comment in neutralCorpus:\n",
    "#    vector = comment_to_vector(comment, list(known_words), M)\n",
    "#    neutral_comment_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1944998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#hateful_comment_vectors = np.array(hateful_comment_vectors)\n",
    "#nHateful_comment_vectors = np.array(nHateful_comment_vectors)\n",
    "#neutral_comment_vectors = np.array(neutral_comment_vectors)\n",
    "\n",
    "#all_comment_vectors = np.concatenate((hateful_comment_vectors, nHateful_comment_vectors, neutral_comment_vectors))\n",
    "\n",
    "#hateful_labels = np.ones(len(hatefulCorpus))\n",
    "#nHateful_labels = np.zeros(len(nHatefulCorpus))\n",
    "#neutral_labels = np.full(len(neutralCorpus), 2)\n",
    "\n",
    "#all_labels = np.concatenate((hateful_labels, nHateful_labels, neutral_labels))\n",
    "\n",
    "#num_classes = 3\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(all_comment_vectors, all_labels, test_size=0.2, random_state=42)\n",
    "#y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "#y_test_one_hot = to_categorical(y_test, num_classes)\n",
    "\n",
    "#embedding_dim = 40\n",
    "\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(input_dim=len(known_words) + 1, output_dim=embedding_dim, input_length=M))\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(units=50, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "#model.fit(X_train, y_train_one_hot, epochs=20, batch_size=32, validation_data=(X_test, y_test_one_hot), callbacks=[early_stopping])\n",
    "\n",
    "#model.save('savedModels/CNN_Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dde71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import keras\n",
    "#testData = \"All immigrants should be deported to where they came from.\"\n",
    "#testData = testData.strip()\n",
    "#testData = replace_emojis(testData, emoji_dict)\n",
    "#testData = preprocess_text(testData)\n",
    "#testDataVector = comment_to_vector(testData, list(known_words), 40)\n",
    "#testDataVector = np.array(testDataVector)\n",
    "#testDataVector = np.expand_dims(testDataVector, axis=0)\n",
    "#loadedModel = tf.keras.models.load_model('savedModels/CNN_Model')\n",
    "#predicted_probabilities = loadedModel.predict(testDataVector)\n",
    "#predicted_class = np.argmax(predicted_probabilities)\n",
    "#class_labels = ['hateful', 'non-hateful', 'neutral']\n",
    "#print(class_labels[predicted_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7757e",
   "metadata": {},
   "source": [
    "## Implementing CNN using Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6125e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 1s 16ms/step - loss: 0.9934 - accuracy: 0.4957 - val_loss: 0.8912 - val_accuracy: 0.6463\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.9299 - accuracy: 0.5726 - val_loss: 0.8753 - val_accuracy: 0.6463\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.9078 - accuracy: 0.5778 - val_loss: 0.8674 - val_accuracy: 0.6463\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.8955 - accuracy: 0.5795 - val_loss: 0.8667 - val_accuracy: 0.6463\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.8847 - accuracy: 0.5795 - val_loss: 0.8671 - val_accuracy: 0.6463\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.8734 - accuracy: 0.5829 - val_loss: 0.8628 - val_accuracy: 0.6395\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.8663 - accuracy: 0.5795 - val_loss: 0.8604 - val_accuracy: 0.6395\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.8563 - accuracy: 0.5795 - val_loss: 0.8511 - val_accuracy: 0.6463\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.8470 - accuracy: 0.5897 - val_loss: 0.8421 - val_accuracy: 0.6463\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.8377 - accuracy: 0.5829 - val_loss: 0.8445 - val_accuracy: 0.6463\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.8205 - accuracy: 0.5863 - val_loss: 0.8418 - val_accuracy: 0.6463\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.8271 - accuracy: 0.5897 - val_loss: 0.8381 - val_accuracy: 0.6259\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7936 - accuracy: 0.6103 - val_loss: 0.8287 - val_accuracy: 0.6395\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7906 - accuracy: 0.6137 - val_loss: 0.8298 - val_accuracy: 0.6259\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.7771 - accuracy: 0.6239 - val_loss: 0.8304 - val_accuracy: 0.6395\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.7514 - accuracy: 0.6274 - val_loss: 0.8255 - val_accuracy: 0.6327\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7375 - accuracy: 0.6376 - val_loss: 0.8231 - val_accuracy: 0.6259\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.7151 - accuracy: 0.6479 - val_loss: 0.8156 - val_accuracy: 0.6259\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6928 - accuracy: 0.6530 - val_loss: 0.8166 - val_accuracy: 0.6327\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6627 - accuracy: 0.6615 - val_loss: 0.8146 - val_accuracy: 0.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedModels/CNN_Model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedModels/CNN_Model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.89      0.78        95\n",
      "           1       0.37      0.18      0.24        39\n",
      "           2       0.50      0.23      0.32        13\n",
      "\n",
      "    accuracy                           0.65       147\n",
      "   macro avg       0.52      0.43      0.45       147\n",
      "weighted avg       0.59      0.65      0.60       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "#Download Glove from http://nlp.stanford.edu/data/glove.6B.zip\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(hatefulCorpus + nHatefulCorpus + neutralCorpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(hatefulCorpus + nHatefulCorpus + neutralCorpus)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y = np.array([0] * len(hatefulCorpus) + [1] * len(nHatefulCorpus) + [2] * len(neutralCorpus))\n",
    "y = to_categorical(y, num_classes=3)\n",
    "size = len(word_index)\n",
    "embedding_matrix = np.zeros((size + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_model(optimizer='adam', filters=32, kernel_size=3, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(size + 1, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Conv1D(filters*2, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(filters*2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#param_grid = {\n",
    "#    'optimizer': ['adam', 'rmsprop'],\n",
    "#    'filters': [16, 32, 64, 128],\n",
    "#    'kernel_size': [3, 5],\n",
    "#    'dropout_rate': [0.3, 0.5]\n",
    "#}\n",
    "\n",
    "#grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "#grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best Parameters: \", grid_result.best_params_)\n",
    "#print(\"Best Score: \", grid_result.best_score_)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model = create_model(optimizer='adam', filters=16, kernel_size=3, dropout_rate=0.3)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[early_stopping])\n",
    "model.save('savedModels/CNN_Model')\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "report = classification_report(y_true, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "625f3e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "The predicted class for 'All immigrants should be deported to where they came from.' is: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "testText = \"All immigrants should be deported to where they came from.\"\n",
    "testSequence = tokenizer.texts_to_sequences([testText])\n",
    "testSequence = pad_sequences(testSequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "loadedModel = tf.keras.models.load_model('savedModels/CNN_Model')\n",
    "prediction = loadedModel.predict(testSequence)\n",
    "predictedClass = np.argmax(prediction)\n",
    "print(f\"The predicted class for '{testText}' is: {predictedClass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109fb6fc",
   "metadata": {},
   "source": [
    "#### Storing Vector Data in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9eb00d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    1, ...,  414, 1330, 1331],\n",
       "       [   0,    0,    0, ...,  416,  239,   10],\n",
       "       [   0,    0,    0, ...,  155,   11,  509],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   23, 2863, 2864],\n",
       "       [   0,    0,    0, ...,  501,   97,  410],\n",
       "       [   0,    0,    0, ...,  155,  910,   62]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Data from training \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2935d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establishing Connection\n",
    "client= MongoClient(\"mongodb://localhost:27017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97ad6fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Database\n",
    "db=client.get_database('Vector_Data')\n",
    "#Get Collection and preprocessed_data is table name inside db\n",
    "collection=db.preprocessed_data\n",
    "# initial number of elements\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7062dd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inserting all elements in preprocessed_data \n",
    "for row in X:\n",
    "    document = {'vector':row.tolist()}\n",
    "    collection.insert_one(document)\n",
    "# insertion check\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf652fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    1, ...,  414, 1330, 1331],\n",
       "       [   0,    0,    0, ...,  416,  239,   10],\n",
       "       [   0,    0,    0, ...,  155,   11,  509],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   23, 2863, 2864],\n",
       "       [   0,    0,    0, ...,  501,   97,  410],\n",
       "       [   0,    0,    0, ...,  155,  910,   62]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting stored vector data from the preprocessed_data\n",
    "vector_data = []\n",
    "alldocuments=collection.find()# here collection is same from preprocessed_data table\n",
    "for document in alldocuments:\n",
    "    vector_data.append(document['vector'])\n",
    "vector_array = np.array(vector_data)\n",
    "#Closing connection\n",
    "client.close()\n",
    "vector_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759598c",
   "metadata": {},
   "source": [
    "Compare the contents of X and vector_array they are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b914ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
